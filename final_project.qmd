---
title: "PPOL670 Final Project"
author: "Ming Zhou & Huixin Cai"
format: html
editor: source
editor_options: 
  chunk_output_type: console
warning: false
self-contained: true
---
# Data Loading
```{r set_up}
# load data
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidymodels)
library(tidyclust)
library(factoextra)
library(broom)
library(ggplot2)
library(parsnip)
library(vip)
data <- read_csv("Data/pppub22.csv")%>%
  select(A_EXPLF,MIG_REG,SPM_NUMKIDS, SPM_ACTC, SPM_BBSUBVAL,
         SPM_FAMTYPE, SPM_CAPHOUSESUB, SPM_CAPWKCCXPNS, SPM_CHILDCAREXPNS,
         A_HGA, SPM_CHILDSUPPD, SPM_EITC, A_MARITL, HEA, SPM_FEDTAX, SPM_GEOADJ,
         SPM_ENGVAL, PRDTRACE, PHIP_VAL, MCAID, NOW_MCAID, NOW_DEPNONM,
         NOW_MRKUN, MRKUN, NOW_MRK, MRK, NOW_DIR, OWNDIR, COV, COV_CYR, AGI, 
         EIT_CRED, CHSP_VAL, CSP_VAL, PAW_VAL, RINT_SC2, RINT_VAL1, PTOTVAL, DIS_HP,
         DIS_CS, DIS_CS, DIS_YN, DIV_YN, DSAB_VAL, PEARNVAL, EARNER, LKWEEKS, LOSEWKS,
         AGE1,A_SEX) %>%
  #exclude observations with missing answers
  filter(complete.cases(.)) %>%
  #only include observations in the labor force (15-64)
  filter(AGE1 > 0 & AGE1 < 15)

# create new variable "in_labor"
# If in the labor force, in_labor equals 1. 
data <- data %>%
  mutate(in_labor = 
           if_else(A_EXPLF == 0,0,1)) %>%
  select(-A_EXPLF)

data$in_labor <- as.factor(data$in_labor)
```

# Supervised modeling without Dimension Reduction 
*Split data*
```{r Split_data}
set.seed(20221111)
split_data <- initial_split(data = data, prop = 0.8)
data_training <- training(x = split_data)
data_testing <- testing(x = split_data)
```

*EDA*
1) The labor participation by gender
```{r}
# The labor participation by gender
data_training %>%
  ggplot(aes(x = in_labor)) +
  geom_bar(aes(fill = factor(A_SEX))) +
  scale_fill_discrete(name = "Sex",
                      labels = c("Male", "Female")) +
  scale_x_discrete(name = "Labor Status",
                   labels = c("Not in labor force", "In labor force")) +
  scale_y_continuous(name = "The number of individuals") +
  labs(title = "Fewer females join the labor force compared to male",
       caption="Data scource: Census.gov")

```
*EDA*
2) The correlation between each variables
```{r}
# The correlation between each variables
data_training1 <- data_training %>%
  select(-in_labor) 

cormat <- round(cor(data_training1),2)
cormat
library(reshape2)
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- 
  ggplot(melted_cormat, 
         aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", 
                       mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 1, 
                                   size = 12, 
                                   hjust = 1))+
  coord_fixed()+
  labs(title = "The correlation between variables",
       caption="Data scource: Census.gov")
# Print the heatmap
print(ggheatmap)
```
*EDA*
3) The distribution of variables
```{r}
#The distribution of SPM_ACTC
ggplot(data = data_training, 
       aes(x = SPM_ACTC)) +
  geom_histogram() +
  labs(title = "The distribution of SPM_ACTC is skewed",
       caption="Data scource: Census.gov")

#The distribution of SPM_CAPWKCCXPNS
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM_CAPWKCCXPNS is skewed",
       caption="Data scource: Census.gov")

#The distribution of SPM_CAPWKCCXPNS
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM_CAPWKCCXPNS is skewed",
       caption="Data scource: Census.gov")
```
*EDA*
4) Some variables are highly correlated
```{r}
ggplot(data = data_training, 
       aes(x = PTOTVAL,
           y = PEARNVAL)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm") +
  labs(title = "PTOTVAL & PEARNVAL are highly correlated",
       caption="Data scource: Census.gov")
```
*Weigh Data*
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these samples a much larger weight in the analysis. 
```{r}
data_training %>% count(in_labor)

data_training <- data_training %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```
*Recipe*
```{r Recipe}
data_rec <- recipe(in_labor~., data = data_training) %>%
  # center and scale all predictors
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  # drop near zero variance predictors
  step_nzv(all_numeric_predictors()) %>%
  step_zv(all_numeric_predictors())
# see the engineered training data
bake(prep(data_rec, training = data_training), new_data = data_training)
```

*Cross validation*
```{r Cross_validation}
folds <- vfold_cv(data = data_training, v = 10)
```

## logistic model
```{r logistic}
#create model
logistic_mod <- 
  logistic_reg(penalty = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification") 

#create workflow
logistic_w <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(logistic_mod) %>%
  add_case_weights(case_wts) 

#perform cross validation 
logistic_cv <- logistic_w %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
logistic_best <- logistic_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
logistic_final <- 
  finalize_workflow(logistic_w,
                    parameters = logistic_best)

logistic_fit <- logistic_final %>%
  fit(data = data_training)

# fit to all training data
predictions_logistic  <- bind_cols(
  data_training,
  predict(object = logistic_fit, 
          new_data = data_training),
  predict(object = logistic_fit, 
          new_data = data_training, type = "prob"))

cm_logistic <- conf_mat(data = predictions_logistic,
               truth = in_labor,
               estimate = .pred_class)
cm_logistic

accuracy(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)
```

## LASSO model 
```{r LASSO}
# create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 50)

# create a linear_regression model so that we can tune the penalty parameter
# set the mixture parameter to 1 and use "glmnet" for the engine
lasso_mod <- 
  logistic_reg(penalty = tune(),
                          mixture = 1) %>%
  set_engine("glmnet")

# create a workflow using logistic regression model
lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lasso_mod) %>%
  add_case_weights(case_wts)

# perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid)

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best)

#fit to the training data 
lasso_fit <- lasso_final %>%
  fit(data = data_training) 

#make predictions
predictions_lasso  <- bind_cols(
  data_training,
  predict(object = lasso_fit, 
          new_data = data_training),
  predict(object = lasso_fit, 
          new_data = data_training, type = "prob"))

cm_lasso <- conf_mat(data = predictions_lasso,
               truth = in_labor,
               estimate = .pred_class)

cm_lasso

accuracy(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare Lasso and Logistic model
The standard error of the LASSO model's accuracy is much lower than the logistic's, and the mean of LASSO model's accuracy is much higher than the logistic's. LASSO model has higher precision in this case. 

```{r Compare_LASSO_logistic}
LASSO_Logistic <- bind_rows(
  `logistic` = show_best(logistic_cv, metric = "accuracy", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "accuracy", n = 1),
  .id = "model"
)

LASSO_Logistic
```

## Decision Tree 
```{r Decision_Tree}
#create model
tree_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(tree_mod)

#perform cross validation 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "rmse" metric
tree_best <- tree_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final <- finalize_workflow(
  tree_wf,
  parameters = tree_best)

#fit to the training data 
tree_fit <- tree_final %>%
  fit(data = data_training) 

#fit to training data
predictions_tree  <- bind_cols(
  data_training,
  predict(object = tree_fit, 
          new_data = data_training),
  predict(object = tree_fit, 
          new_data = data_training, 
          type = "prob"))

select(predictions_tree, in_labor, starts_with(".pred"))

cm_tree <- conf_mat(data = predictions_tree,
               truth = in_labor,
               estimate = .pred_class)
cm_tree

accuracy(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)
```

## Random Forest 
```{r Random_forest}
library(randomForest)
show_engines('rand_forest')

#create model
rf_mod <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#create workflow
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_mod) 

#perform cross validation
rf_cv <- rf_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
rf_best <- rf_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final <- 
  finalize_workflow(rf_wf,
                    parameters = rf_best)

rf_fit <- rf_final %>%
  fit(data = data_training)

# fit to all training data
predictions_rf  <- bind_cols(
  data_training,
  predict(object = rf_fit, 
          new_data = data_training),
  predict(object = rf_fit, 
          new_data = data_training, type = "prob"))

cm_rf <- conf_mat(data = predictions_rf,
               truth = in_labor,
               estimate = .pred_class)
cm_rf

accuracy(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare models 
```{r}
models <- data.frame(
  models = c("logistic","LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.719,0.930,0.930,0.958),
  sensitivity = c(0,0.850,0.851,0.918)
  )

models
```

# Dimension Reduction
## Decide the number of principle components
```{r The_number_of_PC}
data1 <- data %>%
  select(-in_labor)
#create PCA
pca_1 <- prcomp(data1, scale = TRUE)
#create scree plot to decide how many principal components to retain 
screeplot(pca_1, type="lines")
```
The scree plot occurs at component 4, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that **the first three components** should be retained.

## Dimension Reducation with PCA
```{r Dimension_reduction}
# create a recipe with no outcome variable and all predictors
preprocess_rec <- recipe(~ ., data = data1) %>%
  step_zv(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  # center and scale (normalize) all predictors
  step_normalize(all_numeric_predictors())%>%
  # perform pca and use num_comp = 3 to only keep three components
  step_pca(all_numeric_predictors(),num_comp = 3) %>%
  # run prep to prepare recipe
  prep()

# obtain summary metrics (use number = 3)
tidy(preprocess_rec, number = 3, type = "variance")

# obtain loadings (use number = 3)
tidy(preprocess_rec, number = 3, type = "coef")

# apply recipe to data
processed_data <- preprocess_rec %>%
  bake(new_data = data1)

#combine variables
data_pca <- cbind(
  data$in_labor, processed_data
)

# rename variable "data$A_EXPLF"
data_pca <- rename(data_pca,
       "in_labor" = "data$in_labor")
```

# Supervised Modeling with Dimension Reduction
*Split data*
```{r Split_data}
set.seed(20221111)
split_pca <- initial_split(data = data_pca, prop = 0.8)
data_training_pca <- training(x = split_pca)
data_testing_pca <- testing(x = split_pca)
```

*Cross validation*
```{r}
folds_pca <- vfold_cv(data = data_training_pca, v = 10)
```

*Create Recipe*
```{r}
#Build a recipe 
data_rec_pca <- recipe(in_labor~PC1+PC2+PC3, data = data_training_pca) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())
```

## LASSO Model 
```{r}
# create a tuning grid for lasso regularization,
lasso_grid_pca <- grid_regular(penalty(), levels = 50)

# create a logistic_regression model with tuning
lasso_mod_pca <- logistic_reg(
  penalty = tune(), 
  mixture = 1) %>%
  set_engine("glmnet")

# create workflow
lasso_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(lasso_mod_pca) 

# perform hyperparameter tuning 
lasso_cv_pca <- lasso_wf_pca %>%
  tune_grid(
    resamples = folds_pca,
    grid = lasso_grid_pca)

# select the best model based on the "rmse" metric
lasso_best_pca <- lasso_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final_pca <- finalize_workflow(
  lasso_wf_pca,
  parameters = lasso_best_pca)

# fit to the training data and extract coefficients
lasso_fit_pca <- lasso_final_pca %>%
  fit(data = data_training_pca) 

#make predictions
predictions_lasso_pca  <- bind_cols(
  data_training_pca,
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca),
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca, type = "prob"))

cm_lasso_pca <- conf_mat(data = predictions_lasso_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_lasso_pca

accuracy(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Decision Tree 
```{r decision_tree}
#create model
tree_mod_pca <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(tree_mod_pca)

#perform cross validation 
tree_cv_pca <- tree_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
tree_best_pca <- tree_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final_pca <- finalize_workflow(
  tree_wf_pca,
  parameters = tree_best_pca)

#fit to the training data 
tree_fit_pca <- tree_final_pca %>%
  fit(data = data_training_pca) 

#fit to training data
predictions_tree_pca  <- bind_cols(
  data_training_pca,
  predict(object = tree_fit_pca, 
          new_data = data_training_pca),
  predict(object = tree_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

cm_tree_pca <- conf_mat(data = predictions_tree_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_pca

accuracy(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)
#create decision tree graph 
rpart.plot::rpart.plot(x = tree_fit_pca$fit$fit$fit)
```

## Random Forest 
```{r}
#create model
rf_mod_pca <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#create workflow
rf_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(rf_mod_pca) 

#perform cross validation
rf_cv_pca <- rf_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
rf_best_pca <- rf_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final_pca <- 
  finalize_workflow(rf_wf_pca,
                    parameters = rf_best_pca)

rf_fit_pca <- rf_final_pca %>%
  fit(data = data_training_pca)

# make predictions
predictions_rf_pca  <- bind_cols(
  data_training_pca,
  predict(object = rf_fit_pca, 
          new_data = data_training_pca),
  predict(object = rf_fit_pca, 
          new_data = data_training_pca, type = "prob"))

cm_rf_pca <- conf_mat(data = predictions_rf_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_rf_pca

accuracy(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare Models
```{r}
models_pca <- data.frame(
  models = c("LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.803,0.834,0.960),
  sensitivity = c(0.525,0.589,0.904)
  )

models_pca
```

## Choose Decision Tree as our model 
```{r}
# fit to testing data
predictions_tree_test_pca <- bind_cols(
  data_testing_pca,
  predict(object = tree_fit_pca, 
          new_data = data_testing_pca),
  predict(object = tree_fit_pca, 
          new_data = data_testing_pca, type = "prob"))

select(predictions_tree_pca, in_labor, starts_with(".pred"))

cm_tree_test_pca <- conf_mat(data = predictions_tree_test_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_test_pca
```