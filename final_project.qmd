---
title: "PPOL670 Final Project"
author: "Ming Zhou & Huixin Cai"
format: html
editor: source
editor_options: 
  chunk_output_type: console
warning: false
self-contained: true
---
# Data Loading
```{r set_up}
# load data
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidymodels)
library(tidyclust)
library(factoextra)
library(broom)
library(ggplot2)
library(parsnip)
library(vip)
library(reshape2)
data <- read_csv("Data/pppub22.csv")%>%
  select(A_EXPLF,MIG_REG,SPM_NUMKIDS, SPM_ACTC, SPM_BBSUBVAL,
         SPM_FAMTYPE, SPM_CAPHOUSESUB, SPM_CAPWKCCXPNS, SPM_CHILDCAREXPNS,
         A_HGA, SPM_CHILDSUPPD, SPM_EITC, A_MARITL, HEA, SPM_FEDTAX, SPM_GEOADJ,
         SPM_ENGVAL, PRDTRACE, PHIP_VAL, MCAID, NOW_MCAID, NOW_DEPNONM,
         NOW_MRKUN, MRKUN, NOW_MRK, MRK, NOW_DIR, OWNDIR, COV, COV_CYR, AGI, 
         EIT_CRED, CHSP_VAL, CSP_VAL, PAW_VAL, RINT_SC2, RINT_VAL1, PTOTVAL, DIS_HP,
         DIS_CS, DIS_CS, DIS_YN, DIV_YN, DSAB_VAL, PEARNVAL, EARNER, LKWEEKS, LOSEWKS,
         AGE1,A_SEX) %>%
  #exclude observations with missing answers
  filter(complete.cases(.)) %>%
  #only include observations in the labor force (15-64)
  filter(AGE1 > 0 & AGE1 < 15)

# create new variable "in_labor"
# If in the labor force, in_labor equals 1. 
data <- data %>%
  mutate(in_labor = 
           if_else(A_EXPLF == 0,0,1)) %>%
  select(-A_EXPLF)

data$in_labor <- as.factor(data$in_labor)
```

# Supervised modeling without Dimension Reduction 
*Split data*
```{r Split_data}
set.seed(20221111)
split_data <- initial_split(data = data, prop = 0.8)
data_training <- training(x = split_data)
data_testing <- testing(x = split_data)
```

## EDA
### 1) The labor participation by gender
After plotting the labor participation by gender, we find that there are relatively smaller poprotion of the sample not in labor force. And fewer females join the labor force compared to male. 
```{r}
# The labor participation by gender
data_training %>%
  ggplot(aes(x = in_labor)) +
  geom_bar(aes(fill = factor(A_SEX)), position = "dodge") +
  scale_fill_discrete(name = "Sex",
                      labels = c("Male", "Female")) +
  scale_x_discrete(name = "Labor Status",
                   labels = c("Not in labor force", "In labor force")) +
  scale_y_continuous(name = "The number of individuals") +
  labs(title = "Fewer females join the labor force compared to male",
       caption="Data scource: Census.gov")
```

### 2) The correlation between each variables
To visualize the correlation between each variables, we create a heat map. The heat map shows the magnitude of the correlation in two dimensions, and the variation in color is designed by intensity. As shown in the heat map, some variables in the data set are highly correlated. For example, PTOTVAL (total person income) and SPM_FEDTAX (Supplemental Poverty Measure unit's Federal tax) are positively correlated, COV (whether individuals have any health insurance coverage last year) and SPM_CAPWKCCXPNS (Supplemental Poverty Measure unit's child care expenses-not capped) are negatively correlated.  
```{r}
library(reshape2)
library(ggplot2)
# The correlation between each variables
data_training1 <- data_training %>%
  select(-in_labor)

# Generate correlation matrix
cormat <- round(cor(data_training1),1)

# Get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# Get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)

# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- 
  ggplot(melted_cormat, 
         aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", 
                       mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 1, 
                                   size = 12, 
                                   hjust = 1))+
  coord_fixed()+
  labs(title = "The correlation between variables",
       caption="Data scource: Census.gov")

# Print the heatmap
print(ggheatmap)
```

### 3) Some variables are highly correlated
To better visualize the high correlation between variables in the data set, we also use geom_smooth and geom_point. As shown in the graph, educational attainment and total persons income are highly correlated. Individuals with higher education are expected to have higher personal income. 
```{r}
ggplot(data = data_training, 
       aes(x = A_HGA,
           y = PTOTVAL)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "auto") +
  labs(title = "A_HGA & PTOTVAL are highly correlated",
       x = "Educational attainment",
       y= "total persons income", 
       caption="Data scource: Census.gov")
```

### 4) The distribution of variables
After plotting the distribution of some independent variables in our data set, we find that some variables are highly skewed and clustered around some values. 
```{r}
#The distribution of SPM_ACTC (SPM units Additional Child Tax Credit)
ggplot(data = data_training, 
       aes(x = SPM_ACTC)) +
  geom_histogram() +
  labs(title = "The distribution of SPM units Additional Child Tax Credit is skewed and is clustered around 0",
       x = "SPM units Additional Child Tax Credit",
       y = "the number of individuals",
       caption="Data scource: Census.gov")

#The distribution of SPM_CAPWKCCXPNS (SPM unit's capped work and child care expenses)
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM unit's capped work and child care expenses is skewed",
       x = "SPM unit's capped work and child care expenses",
       caption="Data scource: Census.gov")
```

*Weigh Data*
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these samples a much larger weight in the analysis. 
```{r}
data_training %>% count(in_labor)

data_training <- data_training %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```
*Recipe*
```{r Recipe}
data_rec <- recipe(in_labor~., data = data_training) %>%
  # center and scale all predictors
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  # drop near zero variance predictors
  step_nzv(all_numeric_predictors()) %>%
  step_zv(all_numeric_predictors())
# see the engineered training data
bake(prep(data_rec, training = data_training), new_data = data_training)
```

*Cross validation*
```{r Cross_validation}
folds <- vfold_cv(data = data_training, v = 10)
```

## logistic model
```{r logistic}
#create model
logistic_mod <- 
  logistic_reg(penalty = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification") 

#create workflow
logistic_w <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(logistic_mod) %>%
  add_case_weights(case_wts) 

#perform cross validation 
logistic_cv <- logistic_w %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
logistic_best <- logistic_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
logistic_final <- 
  finalize_workflow(logistic_w,
                    parameters = logistic_best)

#fit model to the training data
logistic_fit <- logistic_final %>%
  fit(data = data_training)

#make predictions
predictions_logistic  <- bind_cols(
  data_training,
  predict(object = logistic_fit, 
          new_data = data_training),
  predict(object = logistic_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_logistic <- conf_mat(data = predictions_logistic,
               truth = in_labor,
               estimate = .pred_class)
cm_logistic

accuracy(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
logistic_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## LASSO Model 
```{r LASSO}
# create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 50)

# create a linear_regression model so that we can tune the penalty parameter
# set the mixture parameter to 1 and use "glmnet" for the engine
lasso_mod <- 
  logistic_reg(penalty = tune(),
                          mixture = 1) %>%
  set_engine("glmnet")

# create a workflow using logistic regression model
lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lasso_mod) %>%
  add_case_weights(case_wts)

# perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid)

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best)

#fit model to the training data
lasso_fit <- lasso_final %>%
  fit(data = data_training) 

#make predictions
predictions_lasso  <- bind_cols(
  data_training,
  predict(object = lasso_fit, 
          new_data = data_training),
  predict(object = lasso_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_lasso <- conf_mat(data = predictions_lasso,
               truth = in_labor,
               estimate = .pred_class)

cm_lasso

accuracy(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
lasso_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## Compare Lasso and Logistic model
The standard error of the LASSO model's accuracy is much lower than the logistic's, and the mean of LASSO model's accuracy is much higher than the logistic's. LASSO model has higher precision in this case. 

```{r Compare_LASSO_logistic}
LASSO_Logistic <- bind_rows(
  `logistic` = show_best(logistic_cv, metric = "accuracy", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "accuracy", n = 1),
  .id = "model"
)

LASSO_Logistic
```

## Decision Tree 
```{r Decision_Tree}
#create model
tree_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(tree_mod) %>%
  add_case_weights(case_wts)

#perform cross validation 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "rmse" metric
tree_best <- tree_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final <- finalize_workflow(
  tree_wf,
  parameters = tree_best)

#fit model to the training data
tree_fit <- tree_final %>%
  fit(data = data_training) 

#make predictions 
predictions_tree  <- bind_cols(
  data_training,
  predict(object = tree_fit, 
          new_data = data_training),
  predict(object = tree_fit, 
          new_data = data_training, 
          type = "prob"))

#generate comfusion matrix
cm_tree <- conf_mat(data = predictions_tree,
               truth = in_labor,
               estimate = .pred_class)
cm_tree

accuracy(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
tree_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## Random Forest 
```{r Random_forest}
library(randomForest)
set.seed(20221111)

#create model
rf_mod <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger",importance = "impurity") 

#create workflow
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_mod) %>%
  add_case_weights(case_wts)

#perform cross validation
rf_cv <- rf_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
rf_best <- rf_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final <- 
  finalize_workflow(rf_wf,
                    parameters = rf_best)

#fit model to the training data
rf_fit <- rf_final %>%
  fit(data = data_training)

# make predictions
predictions_rf  <- bind_cols(
  data_training,
  predict(object = rf_fit, 
          new_data = data_training),
  predict(object = rf_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_rf <- conf_mat(data = predictions_rf,
               truth = in_labor,
               estimate = .pred_class)
cm_rf

accuracy(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
ranger_obj <- pull_workflow_fit(rf_fit)$fit
vip(ranger_obj,
    geom = "col")
```

## Compare models 
```{r}
models <- data.frame(
  models = c("logistic","LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.716,0.929,0.922,0.969),
  sensitivity = c(0,0.850,0.879,0.984)
  )

models
```

# Dimension Reduction
## Decide the number of principle components
```{r The_number_of_PC}
data1 <- data %>%
  select(-in_labor)
#create PCA
set.seed(20221111)
pca_1 <- prcomp(data1, scale = TRUE)
#create scree plot to decide how many principal components to retain 
screeplot(pca_1, type="lines")
```
The scree plot occurs at component 4, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that **the first three components** should be retained.

## Dimension Reducation with PCA
```{r Dimension_reduction}
# create a recipe with no outcome variable and all predictors
preprocess_rec <- recipe(~ ., data = data1) %>%
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  # center and scale (normalize) all predictors
  step_normalize(all_numeric_predictors())%>%
  # perform pca and use num_comp = 3 to only keep three components
  step_pca(all_numeric_predictors(),num_comp = 3) %>%
  # run prep to prepare recipe
  prep()

# obtain summary metrics (use number = 3)
tidy(preprocess_rec, number = 3, type = "variance")

# apply recipe to data
processed_data <- preprocess_rec %>%
  bake(new_data = data1)

#combine variables
data_pca <- cbind(
  data$in_labor, processed_data
)

# rename variable "data$A_EXPLF"
data_pca <- rename(data_pca,
       "in_labor" = "data$in_labor")
```

# Supervised Modeling with Dimension Reduction
*Split data*
```{r Split_data}
set.seed(20221111)
split_pca <- initial_split(data = data_pca, prop = 0.8)
data_training_pca <- training(x = split_pca)
data_testing_pca <- testing(x = split_pca)
```

*Weigh Data*
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these samples a much larger weight in the analysis. 
```{r}
data_training_pca %>% count(in_labor)

data_training_pca <- data_training_pca %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```

*Cross validation*
```{r}
folds_pca <- vfold_cv(data = data_training_pca, v = 10)
```

*Create Recipe*
```{r}
#Build a recipe 
data_rec_pca <- recipe(in_labor~PC1+PC2+PC3+case_wts, 
                       data = data_training_pca) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())
```


## LASSO Model with PCA
```{r}
# create a tuning grid for lasso regularization,
lasso_grid_pca <- grid_regular(penalty(), levels = 50)

# create a logistic_regression model with tuning
lasso_mod_pca <- logistic_reg(penalty = tune(), 
                              mixture = 1) %>%
  set_engine("glmnet")

# create workflow
lasso_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(lasso_mod_pca) %>%
  add_case_weights(case_wts) 

# perform hyperparameter tuning 
lasso_cv_pca <- lasso_wf_pca %>%
  tune_grid(
    resamples = folds_pca,
    grid = lasso_grid_pca)

# select the best model based on the "rmse" metric
lasso_best_pca <- lasso_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final_pca <- finalize_workflow(
  lasso_wf_pca,
  parameters = lasso_best_pca)

# fit the model to the training data 
lasso_fit_pca <- lasso_final_pca %>%
  fit(data = data_training_pca) 

#make predictions
predictions_lasso_pca  <- bind_cols(
  data_training_pca,
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca),
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca, type = "prob"))

#generate confusion matrix 
cm_lasso_pca <- conf_mat(data = predictions_lasso_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_lasso_pca 

accuracy(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Decision Tree with PCA
```{r decision_tree}
#create model
tree_mod_pca <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(tree_mod_pca) %>%
  add_case_weights(case_wts) 

#perform cross validation 
tree_cv_pca <- tree_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
tree_best_pca <- tree_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final_pca <- finalize_workflow(
  tree_wf_pca,
  parameters = tree_best_pca)

#fit the model to the training data 
tree_fit_pca <- tree_final_pca %>%
  fit(data = data_training_pca) 

#make predictions
predictions_tree_pca  <- bind_cols(
  data_training_pca,
  predict(object = tree_fit_pca, 
          new_data = data_training_pca),
  predict(object = tree_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

#generate confusion matrix
cm_tree_pca <- conf_mat(data = predictions_tree_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_pca

accuracy(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

#create decision tree graph 
rpart.plot::rpart.plot(x = tree_fit_pca$fit$fit$fit)
```

## Random Forest with PCA
```{r}
set.seed(20221111)
#create model
rf_mod_pca <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

#create workflow
rf_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(rf_mod_pca) %>%
  add_case_weights(case_wts) 

#perform cross validation
rf_cv_pca <- rf_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
rf_best_pca <- rf_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final_pca <- 
  finalize_workflow(rf_wf_pca,
                    parameters = rf_best_pca)

#fit the model to the training data
rf_fit_pca <- rf_final_pca %>%
  fit(data = data_training_pca)

# make predictions
predictions_rf_pca  <- bind_cols(
  data_training_pca,
  predict(object = rf_fit_pca, 
          new_data = data_training_pca),
  predict(object = rf_fit_pca, 
          new_data = data_training_pca, type = "prob"))

#generate confusion matrix 
cm_rf_pca <- conf_mat(data = predictions_rf_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_rf_pca

accuracy(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)
```

# Compare Models
```{r}
models_all <- data.frame(
  models = c("Logistic",
             "LASSO",
             "Decision_Tree",
             "Random_Forest",
             "LASSO_PCA",
             "Decision_Tree_PCA",
             "Random_Forest_PCA"),
  accuracy = c(0.716,0.929,0.922,0.969,
               0.785,0.820,0.961),
  sensitivity = c(0,0.850,0.879,0.984,
                  0.782,0.743,0.999)
  )

highight <- 
  models_all %>%
  filter(models == "Decision_Tree" |
           models == "Random_Forest_PCA")
  
models_all %>%
  ggplot() +  
  geom_text(aes(x = accuracy, 
                y = sensitivity, 
                label = models),
            check_overlap = TRUE,
            nudge_x = -0.005,
            nudge_y = -0.008,
            size = 3,
            angle = 45,
            fontface = "bold") +
  geom_point(aes(x = accuracy, y = sensitivity),
             size = 3) +
  geom_point(data = highight,
             aes(x = accuracy, y = sensitivity),
             color = "red",
             size = 3) +
  labs(title = "Decision tree and Random forest with PCA have relatively high accuracy and sensitivity")
```

## Final model 1: Random Forest with PCA 
```{r}
# fit the model to the testing data and make predictions
predictions_rf_test_pca <- bind_cols(
  data_testing_pca,
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca),
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca, type = "prob"))

#generate confusion matrix 
cm_rf_test_pca <- conf_mat(
  data = predictions_rf_test_pca,
  truth = in_labor,
  estimate = .pred_class)

cm_rf_test_pca

accuracy(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

#plot distributions of the predicted probability distributions for each class 
predictions_rf_test_pca %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = "Sensitivity is high in the model of Random forest with PCA",
       x = "The probability of being predicted not in labor force") 
```

## Final model 2: Decision Tree 
```{r}
# fit the model to the testing data and make predictions
predictions_tree_test <- bind_cols(
  data_testing,
  predict(object = tree_fit, 
          new_data = data_testing),
  predict(object = tree_fit, 
          new_data = data_testing, type = "prob"))

#generate confusion matrix 
cm_tree_test <- conf_mat(data = predictions_tree_test,
               truth = in_labor,
               estimate = .pred_class)

cm_tree_test

accuracy(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

#plot distributions of the predicted probability distributions for each class 
predictions_tree_test %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = "Sensitivity and Accuracy are both significantly high in the Decision Tree model",
       x = "The probability of being predicted not in labor force") 
```