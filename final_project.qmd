---
title: "PPOL670 Final Project"
author: "Ming Zhou & Huixin Cai"
format: html
editor: source
editor_options: 
  chunk_output_type: console
warning: false
self-contained: true
---
# Data Loading
```{r set_up}
# load data
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidymodels)
library(tidyclust)
library(factoextra)
library(broom)
library(ggplot2)
library(parsnip)
library(vip)
data <- read_csv("Data/pppub22.csv") %>%
  select(A_EXPLF,MIG_REG,SPM_NUMKIDS, SPM_ACTC, SPM_BBSUBVAL,
         SPM_FAMTYPE, SPM_CAPHOUSESUB, SPM_CAPWKCCXPNS, SPM_CHILDCAREXPNS,
         A_HGA, SPM_CHILDSUPPD, SPM_EITC, A_MARITL, HEA, SPM_FEDTAX, SPM_GEOADJ,
         SPM_ENGVAL, PRDTRACE, PHIP_VAL, MCAID, NOW_MCAID, NOW_DEPNONM,
         NOW_MRKUN, MRKUN, NOW_MRK, MRK, NOW_DIR, OWNDIR, COV, COV_CYR, AGI, 
         EIT_CRED, CHSP_VAL, CSP_VAL, PAW_VAL, RINT_SC2, RINT_VAL1, PTOTVAL, DIS_HP,
         DIS_CS, DIS_CS, DIS_YN, DIV_YN, DSAB_VAL, PEARNVAL, EARNER, LKWEEKS, LOSEWKS,
         AGE1,A_SEX) %>%
  #exclude observations with missing answers
  filter(complete.cases(.)) %>%
  #only include observations in the labor force (15-64)
  filter(AGE1 > 0 & AGE1 < 15)

# create new variable "in_labor"
# If in the labor force, in_labor equals 1. 
data <- data %>%
  mutate(in_labor = 
           if_else(A_EXPLF == 0,0,1)) %>%
  select(-A_EXPLF)

data$in_labor <- as.factor(data$in_labor)
```

# Supervised modeling without Dimension Reduction 
*Split data*
```{r Split_data}
split_data <- initial_split(data = data, prop = 0.8)
data_training <- training(x = split_data)
data_testing <- testing(x = split_data)
```
*EDA*
1) The labor participation by gender
```{r}
# The labor participation by gender
data_training %>%
  ggplot(aes(x = in_labor)) +
  geom_bar(aes(fill = factor(A_SEX))) +
  scale_fill_discrete(name = "Sex",
                      labels = c("Male", "Female")) +
  scale_x_discrete(name = "Labor Status",
                   labels = c("Not in labor force", "In labor force")) +
  scale_y_continuous(name = "The number of individuals") +
  labs(title = "Fewer females join the labor force compared to male",
       caption="Data scource: Census.gov")

```
*EDA*
2) The correlation between each variables
```{r}
# The correlation between each variables
data_training1 <- data_training %>%
  select(-in_labor) 

cormat <- round(cor(data_training1),2)
cormat
library(reshape2)
reorder_cormat <- function(cormat){
# Use correlation between variables as distance
dd <- as.dist((1-cormat)/2)
hc <- hclust(dd)
cormat <-cormat[hc$order, hc$order]
}
# Reorder the correlation matrix
cormat <- reorder_cormat(cormat)
upper_tri <- get_upper_tri(cormat)
# Melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# Create a ggheatmap
ggheatmap <- 
  ggplot(melted_cormat, 
         aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", 
                       mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 1, 
                                   size = 12, 
                                   hjust = 1))+
  coord_fixed()+
  labs(title = "The correlation between variables",
       caption="Data scource: Census.gov")
# Print the heatmap
print(ggheatmap)
```
*EDA*
3) The distribution of variables
```{r}
#The distribution of SPM_ACTC
ggplot(data = data_training, 
       aes(x = SPM_ACTC)) +
  geom_histogram() +
  labs(title = "The distribution of SPM_ACTC is skewed",
       caption="Data scource: Census.gov")

#The distribution of SPM_CAPWKCCXPNS
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM_CAPWKCCXPNS is skewed",
       caption="Data scource: Census.gov")

#The distribution of SPM_CAPWKCCXPNS
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM_CAPWKCCXPNS is skewed",
       caption="Data scource: Census.gov")
```
*EDA*
4) Some variables are highly correlated
```{r}
ggplot(data = data_training, 
       aes(x = PTOTVAL,
           y = PEARNVAL)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "lm") +
  labs(title = "PTOTVAL & PEARNVAL are highly correlated",
       caption="Data scource: Census.gov")
```
*Recipe*
```{r Recipe}
data_rec <- recipe(in_labor~., data = data_training) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors()) %>%
  # drop near zero variance predictors
  step_nzv(all_predictors()) %>%
  step_zv(all_predictors())
# see the engineered training data
bake(prep(data_rec, training = data_training), new_data = data_training)
```

*Cross validation*
```{r Cross_validation}
set.seed(20221111)
folds <- vfold_cv(data = data_training, v = 10)
```

## logistic model
```{r logistic}
#create model
logistic_mod <- 
  logistic_reg(penalty = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification")

#create workflow
logistic_w <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(logistic_mod) 

#perform cross validation 
logistic_cv <- logistic_w %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
logistic_best <- logistic_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
logistic_final <- 
  finalize_workflow(logistic_w,
                    parameters = logistic_best)

logistic_fit <- logistic_final %>%
  fit(data = data_training)

# fit to all training data
predictions_logistic  <- bind_cols(
  data_training,
  predict(object = logistic_fit, 
          new_data = data_training),
  predict(object = logistic_fit, 
          new_data = data_training, type = "prob"))

cm_logistic <- conf_mat(data = predictions_logistic,
               truth = in_labor,
               estimate = .pred_class)
cm_logistic

accuracy(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)
```

## LASSO model 
```{r LASSO}
# create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 50)

# create a linear_regression model so that we can tune the penalty parameter
# set the mixture parameter to 1 and use "glmnet" for the engine
lasso_mod <- logistic_reg(penalty = tune(),
                          mixture = 1) %>%
  set_engine("glmnet")

# create a workflow using logistic regression model
lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lasso_mod) 

# perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid)

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best)

#fit to the training data 
lasso_fit <- lasso_final %>%
  fit(data = data_training) 

#make predictions
predictions_lasso  <- bind_cols(
  data_training,
  predict(object = lasso_fit, 
          new_data = data_training),
  predict(object = lasso_fit, 
          new_data = data_training, type = "prob"))

cm_lasso <- conf_mat(data = predictions_lasso,
               truth = in_labor,
               estimate = .pred_class)

cm_lasso

accuracy(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare Lasso and Logistic model
```{r Compare_LASSO_logistic}
LASSO_Logistic <- bind_rows(
  `logistic` = show_best(logistic_cv, metric = "accuracy", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "accuracy", n = 1),
  .id = "model"
)

LASSO_Logistic
```

## Decision Tree 
```{r Decision_Tree}
#create model
tree_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(tree_mod)

#perform cross validation 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "rmse" metric
tree_best <- tree_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final <- finalize_workflow(
  tree_wf,
  parameters = tree_best)

#fit to the training data 
tree_fit <- tree_final %>%
  fit(data = data_training) 

#fit to training data
predictions_tree  <- bind_cols(
  data_training,
  predict(object = tree_fit, 
          new_data = data_training),
  predict(object = tree_fit, 
          new_data = data_training, 
          type = "prob"))

select(predictions_tree, in_labor, starts_with(".pred"))

cm_tree <- conf_mat(data = predictions_tree,
               truth = in_labor,
               estimate = .pred_class)
cm_tree

accuracy(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)
```
## Random Forest 
```{r Random_forest}
library(randomForest)
show_engines('rand_forest')

#create model
rf_mod <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#create workflow
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_mod) 

#perform cross validation
rf_cv <- rf_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
rf_best <- rf_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final <- 
  finalize_workflow(rf_wf,
                    parameters = rf_best)

rf_fit <- rf_final %>%
  fit(data = data_training)

# fit to all training data
predictions_rf  <- bind_cols(
  data_training,
  predict(object = rf_fit, 
          new_data = data_training),
  predict(object = rf_fit, 
          new_data = data_training, type = "prob"))

cm_rf <- conf_mat(data = predictions_rf,
               truth = in_labor,
               estimate = .pred_class)
cm_rf

accuracy(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare models 
```{r}
models <- data.frame(
  models = c("logistic","LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.719,0.930,0.930,0.958),
  sensitivity = c(0,0.850,0.851,0.918)
  )

models
```

# Dimension Reduction
## Decide the number of principle components
```{r The_number_of_PC}
data1 <- data %>%
  select(-in_labor)
#create PCA
pca_1 <- prcomp(data1, scale = TRUE)
#create scree plot to decide how many principal components to retain 
screeplot(pca_1, type="lines")
```
The scree plot occurs at component 4, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that **the first three components** should be retained.

## Dimension Reducation with PCA
```{r Dimension_reduction}
# create a recipe with no outcome variable and all predictors
preprocess_rec <- recipe(~ ., data = data1) %>%
  step_zv(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  # center and scale (normalize) all predictors
  step_normalize(all_numeric_predictors())%>%
  # perform pca and use num_comp = 3 to only keep three components
  step_pca(all_numeric_predictors(),num_comp = 3) %>%
  # run prep to prepare recipe
  prep()

# obtain summary metrics (use number = 3)
tidy(preprocess_rec, number = 3, type = "variance")

# obtain loadings (use number = 3)
tidy(preprocess_rec, number = 3, type = "coef")

# apply recipe to data
processed_data <- preprocess_rec %>%
  bake(new_data = data1)

#combine variables
data_pca <- cbind(
  data$in_labor, processed_data
)

# rename variable "data$A_EXPLF"
data_pca <- rename(data_pca,
       "in_labor" = "data$in_labor")
```

# Supervised Modeling with Dimension Reduction
## Split data 
```{r Split_data}
split_pca <- initial_split(data = data_pca, prop = 0.8)
data_training_pca <- training(x = split_pca)
data_testing_pca <- testing(x = split_pca)
```

## Cross validation 
```{r}
set.seed(20221111)
folds_pca <- vfold_cv(data = data_training_pca, v = 10)
```

## Create Recipe
```{r}
#Build a recipe 
data_rec_pca <- recipe(in_labor~PC1+PC2+PC3, data = data_training_pca) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())
```

## LASSO Model 
```{r}
# create a tuning grid for lasso regularization,
lasso_grid_pca <- grid_regular(penalty(), levels = 50)

# create a logistic_regression model with tuning
lasso_mod_pca <- logistic_reg(
  penalty = tune(), 
  mixture = 1) %>%
  set_engine("glmnet")

# create workflow
lasso_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(lasso_mod_pca) 

# perform hyperparameter tuning 
lasso_cv_pca <- lasso_wf_pca %>%
  tune_grid(
    resamples = folds_pca,
    grid = lasso_grid_pca)

# select the best model based on the "rmse" metric
lasso_best_pca <- lasso_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final_pca <- finalize_workflow(
  lasso_wf_pca,
  parameters = lasso_best_pca)

# fit to the training data and extract coefficients
lasso_fit_pca <- lasso_final_pca %>%
  fit(data = data_training_pca) 

#make predictions
predictions_lasso_pca  <- bind_cols(
  data_training_pca,
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca),
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca, type = "prob"))

cm_lasso_pca <- conf_mat(data = predictions_lasso_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_lasso_pca

accuracy(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Decision Tree 
```{r decision_tree}
#create model
tree_mod_pca <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(tree_mod_pca)

#perform cross validation 
tree_cv_pca <- tree_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
tree_best_pca <- tree_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final_pca <- finalize_workflow(
  tree_wf_pca,
  parameters = tree_best_pca)

#fit to the training data 
tree_fit_pca <- tree_final_pca %>%
  fit(data = data_training_pca) 

#fit to training data
predictions_tree_pca  <- bind_cols(
  data_training_pca,
  predict(object = tree_fit_pca, 
          new_data = data_training_pca),
  predict(object = tree_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

cm_tree_pca <- conf_mat(data = predictions_tree_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_pca

accuracy(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)
#create decision tree graph 
rpart.plot::rpart.plot(x = tree_fit_pca$fit$fit$fit)
```

## Random Forest 
```{r}
#create model
rf_mod_pca <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger")

#create workflow
rf_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(rf_mod_pca) 

#perform cross validation
rf_cv_pca <- rf_wf_pca %>%
  fit_resamples(resamples = folds_pca)

#select the best model based on the "accuracy" metric
rf_best_pca <- rf_cv_pca %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final_pca <- 
  finalize_workflow(rf_wf_pca,
                    parameters = rf_best_pca)

rf_fit_pca <- rf_final_pca %>%
  fit(data = data_training_pca)

# make predictions
predictions_rf_pca  <- bind_cols(
  data_training_pca,
  predict(object = rf_fit_pca, 
          new_data = data_training_pca),
  predict(object = rf_fit_pca, 
          new_data = data_training_pca, type = "prob"))

cm_rf_pca <- conf_mat(data = predictions_rf_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_rf_pca

accuracy(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Compare Models
```{r}
models_pca <- data.frame(
  models = c("LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.803,0.834,0.960),
  sensitivity = c(0.525,0.589,0.904)
  )

models_pca


```

## Choose Decision Tree as our model 
```{r}
# fit to testing data
predictions_tree_test_pca <- bind_cols(
  data_testing_pca,
  predict(object = tree_fit_pca, 
          new_data = data_testing_pca),
  predict(object = tree_fit_pca, 
          new_data = data_testing_pca, type = "prob"))

select(predictions_tree_pca, in_labor, starts_with(".pred"))

cm_tree_test_pca <- conf_mat(data = predictions_tree_test_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_test_pca
```