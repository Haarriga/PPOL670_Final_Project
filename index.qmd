---
title: "PPOL670 Final Project"
author: "Ming Zhou & Huixin Cai"
format: html
editor: source
editor_options: 
  chunk_output_type: console
warning: false
self-contained: true
---
# Project Purpose
In this project, we aim to predict whether an individual is in the labor force based on their demographic and economic background characteristics. Hopefully, the model developed in this project can help policymakers identify individuals not in the labor force and provide them with guidance and potential economic support. We expect policymakers to develop a supporting program that identifies those not in the labor force, and 1) interview randomly selected individuals to know better about why they stay out of the labor force; 2) adjust the general budget for job training and economic support programs.
We perform supervised machine learning using multiple models (LASSO, Decision Tree, Random Forest, etc.) both with and without PCA. As our data set is unbalanced, we create a weight variable for adjustment.

# Data
We gathered the variables needed to train the model from the Annual Social and Economic (ASEC) supplement data. The independent variables include individuals’ demographics (e.g., age, education level, marital status, sex, etc.) and other economic status information (e.g., SNAP benefits, EITC benefits, and other social assistance). The Annual Social and Economic (ASEC) Supplement contains the basic monthly demographic and labor force data, plus additional data on work experience, income, noncash benefits, health insurance coverage, and migration. We can access the data through web APIs as the data dictionary linked https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar22.pdf.

```{r set_up}
# load data
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidymodels)
library(tidyclust)
library(factoextra)
library(broom)
library(ggplot2)
library(parsnip)
library(vip)
library(reshape2)
data <- read_csv("Data/pppub22.csv")%>%
  select(A_EXPLF,MIG_REG,SPM_NUMKIDS, SPM_ACTC, SPM_BBSUBVAL,
         SPM_FAMTYPE, SPM_CAPHOUSESUB, SPM_CAPWKCCXPNS, SPM_CHILDCAREXPNS,
         A_HGA, SPM_CHILDSUPPD, SPM_EITC, A_MARITL, HEA, SPM_FEDTAX, SPM_GEOADJ,
         SPM_ENGVAL, PRDTRACE, PHIP_VAL, MCAID, NOW_MCAID, NOW_DEPNONM,
         NOW_MRKUN, MRKUN, NOW_MRK, MRK, NOW_DIR, OWNDIR, COV, COV_CYR, AGI, 
         EIT_CRED, CHSP_VAL, CSP_VAL, PAW_VAL, RINT_SC2, RINT_VAL1, PTOTVAL, DIS_HP,
         DIS_CS, DIS_CS, DIS_YN, DIV_YN, DSAB_VAL, PEARNVAL, EARNER, LKWEEKS, LOSEWKS,
         AGE1,A_SEX) %>%
  # exclude observations with missing answers
  filter(complete.cases(.)) %>%
  # only include observations in the labor force (15-64)
  filter(AGE1 > 0 & AGE1 < 15)

# create new variable "in_labor"
# if in the labor force, in_labor equals 1. 
data <- data %>%
  mutate(in_labor = 
           if_else(A_EXPLF == 0,0,1)) %>%
  select(-A_EXPLF)

data$in_labor <- as.factor(data$in_labor)
```

# Supervised modeling without Dimension Reduction 
*Split data*
```{r Split_data}
set.seed(20221111)
split_data <- initial_split(data = data, prop = 0.8)
data_training <- training(x = split_data)
data_testing <- testing(x = split_data)
```

## EDA
### 1) The labor participation by gender
After plotting the labor participation by gender, we find that there are relatively smaller proportion of the sample not in labor force. And fewer females join the labor force compared to male. 
```{r labor_participation_by_gender}
# the labor participation by gender
data_training %>%
  ggplot(aes(x = in_labor)) +
  geom_bar(aes(fill = factor(A_SEX)), position = "dodge") +
  scale_fill_discrete(name = "Sex",
                      labels = c("Male", "Female")) +
  scale_x_discrete(name = "Labor Status",
                   labels = c("Not in labor force", "In labor force")) +
  scale_y_continuous(name = "The number of individuals") +
  labs(title = "Fewer females join the labor force compared to male",
       caption="Data scource: Census.gov")
```

### 2) The correlation between each variables
To visualize the correlation between each variables, we create a heat map. The heat map shows the magnitude of the correlation in two dimensions, and the variation in color is designed by intensity. As shown in the heat map, some variables in the data set are highly correlated. For example, PTOTVAL (total person income) and SPM_FEDTAX (Supplemental Poverty Measure unit's Federal tax) are positively correlated, COV (whether individuals have any health insurance coverage last year) and SPM_CAPWKCCXPNS (Supplemental Poverty Measure unit's child care expenses-not capped) are negatively correlated.  
```{r correlation}
library(reshape2)
library(ggplot2)
# the correlation between each variables
data_training1 <- data_training %>%
  select(-in_labor)

# generate correlation matrix
cormat <- round(cor(data_training1),1)

# get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)

# melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# create a ggheatmap
ggheatmap <- 
  ggplot(melted_cormat, 
         aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", 
                       mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 1, 
                                   size = 12, 
                                   hjust = 1))+
  coord_fixed()+
  labs(title = "The correlation between variables",
       caption="Data scource: Census.gov")

# print the heatmap
print(ggheatmap)
```

### 3) Some variables are highly correlated
To better visualize the high correlation between variables in the data set, we also use geom_smooth and geom_point. As shown in the graph, educational attainment and total persons income are highly correlated. Individuals with higher education are expected to have higher personal income. 
```{r highly_correlated}
ggplot(data = data_training, 
       aes(x = A_HGA,
           y = PTOTVAL)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "auto") +
  labs(title = "A_HGA & PTOTVAL are highly correlated",
       x = "Educational attainment",
       y= "total persons income", 
       caption="Data scource: Census.gov")
```

### 4) The distribution of variables
After plotting the distribution of some independent variables in our data set, we find that some variables are highly skewed and clustered around some values. 
```{r distribution}
# the distribution of SPM_ACTC (SPM units Additional Child Tax Credit)
ggplot(data = data_training, 
       aes(x = SPM_ACTC)) +
  geom_histogram() +
  labs(title = "The distribution of SPM units Additional Child Tax Credit is skewed and is clustered around 0",
       x = "SPM units Additional Child Tax Credit",
       y = "the number of individuals",
       caption="Data scource: Census.gov")

# the distribution of SPM_CAPWKCCXPNS (SPM unit's capped work and child care expenses)
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram() +
  scale_x_continuous(limits=c(0,50000,100000))+
  labs(title = "The distribution of SPM unit's capped work and child care expenses is skewed",
       x = "SPM unit's capped work and child care expenses",
       caption="Data scource: Census.gov")
```

## Preprocess
### Add weight
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these observations a much larger weight in the analysis. 
```{r weight}
data_training %>% count(in_labor)

data_training <- data_training %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```

### Recipe
To reduce standard error in the models, we use standardization, removing variables that have large absolute correlations with other variables, and dropping variables with no variance in the pre-process.
```{r recipe}
data_rec <- recipe(in_labor~., data = data_training) %>%
  # center and scale all predictors
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  # drop near zero variance predictors
  step_nzv(all_numeric_predictors()) %>%
  step_zv(all_numeric_predictors())
# see the engineered training data
bake(prep(data_rec, training = data_training), new_data = data_training)
```

### Cross validation
```{r cross_validation}
folds <- vfold_cv(data = data_training, v = 10)
```

## Logistic model
The accuracy and sensitivity of the logitstic model is 0.716 and 0, respectively. The logistic model does not fit well in this case. 
```{r logistic}
# create model
logistic_mod <- 
  logistic_reg(penalty = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification") 

# create workflow
logistic_w <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(logistic_mod) %>%
  add_case_weights(case_wts) 

# perform cross validation 
logistic_cv <- logistic_w %>%
  fit_resamples(resamples = folds)

# select the best model based on the "accuracy" metric
logistic_best <- logistic_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
logistic_final <- 
  finalize_workflow(logistic_w,
                    parameters = logistic_best)

# fit model to the training data
logistic_fit <- logistic_final %>%
  fit(data = data_training)

# make predictions
predictions_logistic  <- bind_cols(
  data_training,
  predict(object = logistic_fit, 
          new_data = data_training),
  predict(object = logistic_fit, 
          new_data = data_training, type = "prob"))

# generate confusion matrix
cm_logistic <- conf_mat(data = predictions_logistic,
               truth = in_labor,
               estimate = .pred_class)
cm_logistic

accuracy(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

# variable importance 
logistic_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## LASSO Model 
The accuracy and sensitivity of the LASSO model is 0.929 and 0.850, respectively. 
```{r LASSO}
# create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 50)

# create a linear_regression model so that we can tune the penalty parameter
# set the mixture parameter to 1 and use "glmnet" for the engine
lasso_mod <- 
  logistic_reg(penalty = tune(),
                          mixture = 1) %>%
  set_engine("glmnet")

# create a workflow using logistic regression model
lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lasso_mod) %>%
  add_case_weights(case_wts)

# perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid)

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best)

#fit model to the training data
lasso_fit <- lasso_final %>%
  fit(data = data_training) 

#make predictions
predictions_lasso  <- bind_cols(
  data_training,
  predict(object = lasso_fit, 
          new_data = data_training),
  predict(object = lasso_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_lasso <- conf_mat(data = predictions_lasso,
               truth = in_labor,
               estimate = .pred_class)

cm_lasso

accuracy(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
lasso_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## Compare LASSO and logistic model
The standard error of the LASSO model's accuracy is much lower than the logistic's, and the mean of LASSO model's accuracy is much higher than the logistic's. The LASSO model is far better than the logistic model, fitting well in the training data with high accuracy and sensitivity. 

LASSO model has higher precision in this case. 

```{r compare_LASSO_logistic}
LASSO_Logistic <- bind_rows(
  `logistic` = show_best(logistic_cv, metric = "accuracy", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "accuracy", n = 1),
  .id = "model"
)

LASSO_Logistic
```

## Decision Tree 
The accuracy and sensitivity of the Decision Tree model is 0.922 and 0.879, respectively. The Decision Tree model fits the training data as well as LASSO model, with slightly higher sensitivity.  
```{r decision_tree}
#create model
tree_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(tree_mod) %>%
  add_case_weights(case_wts)

#perform cross validation 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "rmse" metric
tree_best <- tree_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final <- finalize_workflow(
  tree_wf,
  parameters = tree_best)

#fit model to the training data
tree_fit <- tree_final %>%
  fit(data = data_training) 

#make predictions 
predictions_tree  <- bind_cols(
  data_training,
  predict(object = tree_fit, 
          new_data = data_training),
  predict(object = tree_fit, 
          new_data = data_training, 
          type = "prob"))

#generate comfusion matrix
cm_tree <- conf_mat(data = predictions_tree,
               truth = in_labor,
               estimate = .pred_class)
cm_tree

accuracy(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
tree_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10)
```

## Random Forest 
The accuracy and sensitivity of the Random Forest model is 0.969 and 0.984, respectively. The Random Forest model fits the training data best among the four models.
```{r Random_forest}
library(randomForest)
set.seed(20221111)

#create model
rf_mod <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger",importance = "impurity") 

#create workflow
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_mod) %>%
  add_case_weights(case_wts)

#perform cross validation
rf_cv <- rf_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
rf_best <- rf_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final <- 
  finalize_workflow(rf_wf,
                    parameters = rf_best)

#fit model to the training data
rf_fit <- rf_final %>%
  fit(data = data_training)

# make predictions
predictions_rf  <- bind_cols(
  data_training,
  predict(object = rf_fit, 
          new_data = data_training),
  predict(object = rf_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_rf <- conf_mat(data = predictions_rf,
               truth = in_labor,
               estimate = .pred_class)
cm_rf

accuracy(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
ranger_obj <- pull_workflow_fit(rf_fit)$fit
vip(ranger_obj,
    geom = "col")
```

## Compare models 
```{r}
models <- data.frame(
  models = c("logistic","LASSO","Decision_Tree","Random_Forest"),
  accuracy = c(0.716,0.929,0.922,0.969),
  sensitivity = c(0,0.850,0.879,0.984)
  )

models
```

# Dimension Reduction
## Decide the number of principle components
```{r number_of_PC}
data1 <- data %>%
  select(-in_labor)
# create PCA
set.seed(20221111)
pca_1 <- prcomp(data1, scale = TRUE)
# create scree plot to decide how many principal components to retain 
screeplot(pca_1, type="lines")
```
The scree plot occurs at component 4, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that **the first three components** should be retained.

## Dimension Reducation with PCA
```{r dimension_reduction}
# create a recipe with no outcome variable and all predictors
preprocess_rec <- recipe(~ ., data = data1) %>%
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  # center and scale (normalize) all predictors
  step_normalize(all_numeric_predictors())%>%
  # perform pca and use num_comp = 3 to only keep three components
  step_pca(all_numeric_predictors(),num_comp = 3) %>%
  # run prep to prepare recipe
  prep()

# obtain summary metrics (use number = 3)
tidy(preprocess_rec, number = 3, type = "variance")

# apply recipe to data
processed_data <- preprocess_rec %>%
  bake(new_data = data1)

#combine variables
data_pca <- cbind(
  data$in_labor, processed_data
)

# rename variable "data$A_EXPLF"
data_pca <- rename(data_pca,
       "in_labor" = "data$in_labor")
```

# Supervised Modeling with Dimension Reduction
## Preprocess
```{r split_data_pca}
set.seed(20221111)
split_pca <- initial_split(data = data_pca, prop = 0.8)
data_training_pca <- training(x = split_pca)
data_testing_pca <- testing(x = split_pca)
```

### Add weight
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these samples a much larger weight in the analysis. 
```{r weight_pca}
data_training_pca %>% count(in_labor)

data_training_pca <- data_training_pca %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```

### Cross validation
```{r cv_pca}
folds_pca <- vfold_cv(data = data_training_pca, v = 10)
```

### Recipe
To reduce standard error in the models, we use standardization and removing variables that have large absolute correlations with other variables in the pre-process.
```{r rec_pca}
#Build a recipe 
data_rec_pca <- recipe(in_labor~PC1+PC2+PC3+case_wts, 
                       data = data_training_pca) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())
```

## LASSO Model with PCA
The accuracy and sensitivity of the LASSO model with PCA is 0.785 and 0.782, respectively, which fit the training data worse than the LASSO model without PCA.
```{r lasso_pca}
# create a tuning grid for lasso regularization,
lasso_grid_pca <- grid_regular(penalty(), levels = 50)

# create a logistic_regression model with tuning
lasso_mod_pca <- logistic_reg(penalty = tune(), 
                              mixture = 1) %>%
  set_engine("glmnet")

# create workflow
lasso_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(lasso_mod_pca) %>%
  add_case_weights(case_wts) 

# perform hyperparameter tuning 
lasso_cv_pca <- lasso_wf_pca %>%
  tune_grid(
    resamples = folds_pca,
    grid = lasso_grid_pca)

# select the best model based on the "rmse" metric
lasso_best_pca <- lasso_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final_pca <- finalize_workflow(
  lasso_wf_pca,
  parameters = lasso_best_pca)

# fit the model to the training data 
lasso_fit_pca <- lasso_final_pca %>%
  fit(data = data_training_pca) 

# make predictions
predictions_lasso_pca  <- bind_cols(
  data_training_pca,
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca),
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca, type = "prob"))

# generate confusion matrix 
cm_lasso_pca <- conf_mat(data = predictions_lasso_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_lasso_pca 

accuracy(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Decision tree with PCA
The accuracy and sensitivity of the Decision Tree model with PCA is 0.820 and 0.743, respectively, which fit the training data worse than the Decision Tree model without PCA.
```{r decision_tree_pca}
# create model
tree_mod_pca <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create workflow
tree_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(tree_mod_pca) %>%
  add_case_weights(case_wts) 

# perform cross validation 
tree_cv_pca <- tree_wf_pca %>%
  fit_resamples(resamples = folds_pca)

# select the best model based on the "accuracy" metric
tree_best_pca <- tree_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
tree_final_pca <- finalize_workflow(
  tree_wf_pca,
  parameters = tree_best_pca)

# fit the model to the training data 
tree_fit_pca <- tree_final_pca %>%
  fit(data = data_training_pca) 

# make predictions
predictions_tree_pca  <- bind_cols(
  data_training_pca,
  predict(object = tree_fit_pca, 
          new_data = data_training_pca),
  predict(object = tree_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

# generate confusion matrix
cm_tree_pca <- conf_mat(data = predictions_tree_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_pca

accuracy(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

# create decision tree graph 
rpart.plot::rpart.plot(x = tree_fit_pca$fit$fit$fit)
```

## Random Forest with PCA
The accuracy and sensitivity of the Random Forest model with PCA is 0.961 and 0.999, respectively, which fit the training data better than the Random Forest model without PCA.
```{r rf_pca}
set.seed(20221111)
# create model
rf_mod_pca <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# create workflow
rf_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(rf_mod_pca) %>%
  add_case_weights(case_wts) 

# perform cross validation
rf_cv_pca <- rf_wf_pca %>%
  fit_resamples(resamples = folds_pca)

# select the best model based on the "accuracy" metric
rf_best_pca <- rf_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
rf_final_pca <- 
  finalize_workflow(rf_wf_pca,
                    parameters = rf_best_pca)

# fit the model to the training data
rf_fit_pca <- rf_final_pca %>%
  fit(data = data_training_pca)

# make predictions
predictions_rf_pca  <- bind_cols(
  data_training_pca,
  predict(object = rf_fit_pca, 
          new_data = data_training_pca),
  predict(object = rf_fit_pca, 
          new_data = data_training_pca, type = "prob"))

# generate confusion matrix 
cm_rf_pca <- conf_mat(data = predictions_rf_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_rf_pca

accuracy(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)
```

# Compare models
The accuracy and sensitivity of the Random Forest model with PCA is highest among the seven models, as shown in the plot. We choose it as our final model to fit the testing data. 
```{r compare_models}
models_all <- data.frame(
  models = c("Logistic",
             "LASSO",
             "Decision_Tree",
             "Random_Forest",
             "LASSO_PCA",
             "Decision_Tree_PCA",
             "Random_Forest_PCA"),
  accuracy = c(0.716,0.929,0.922,0.969,
               0.785,0.820,0.961),
  sensitivity = c(0,0.850,0.879,0.984,
                  0.782,0.743,0.999)
  )

highight <- 
  models_all %>%
  filter(models == "Decision_Tree" |
           models == "Random_Forest_PCA")
  
models_all %>%
  ggplot() +  
  geom_text(aes(x = accuracy, 
                y = sensitivity, 
                label = models),
            check_overlap = TRUE,
            nudge_x = -0.005,
            nudge_y = -0.008,
            size = 3,
            angle = 45,
            fontface = "bold") +
  geom_point(aes(x = accuracy, y = sensitivity),
             size = 3) +
  geom_point(data = highight,
             aes(x = accuracy, y = sensitivity),
             color = "red",
             size = 3) +
  labs(title = "Decision tree and random forest with PCA have relatively high accuracy and sensitivity")
```

## Final model 1: Random forest with PCA 
The Random Forest Model fits testing data well, as shown in the density plot. The accuracy and sensitivity of the Random Forest model with PCA is relatively high in the testing data, with 0.866 and 0.810 respectively. 
```{r final_model_1}
# fit the model to the testing data and make predictions
predictions_rf_test_pca <- bind_cols(
  data_testing_pca,
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca),
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca, type = "prob"))

#generate confusion matrix 
cm_rf_test_pca <- conf_mat(
  data = predictions_rf_test_pca,
  truth = in_labor,
  estimate = .pred_class)

cm_rf_test_pca

accuracy(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

#plot distributions of the predicted probability distributions for each class 
predictions_rf_test_pca %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = "Sensitivity is high in the model of random forest with PCA",
       x = "The probability of being predicted not in labor force") 
```

## Final model 2: Decision tree 
```{r final_model_2}
# fit the model to the testing data and make predictions
predictions_tree_test <- bind_cols(
  data_testing,
  predict(object = tree_fit, 
          new_data = data_testing),
  predict(object = tree_fit, 
          new_data = data_testing, type = "prob"))

# generate confusion matrix 
cm_tree_test <- conf_mat(data = predictions_tree_test,
               truth = in_labor,
               estimate = .pred_class)

cm_tree_test

accuracy(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

# plot distributions of the predicted probability distributions for each class 
predictions_tree_test %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = "Sensitivity and accuracy are both significantly high in the decision tree model",
       x = "The probability of being predicted not in labor force") 
```
