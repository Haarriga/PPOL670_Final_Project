---
title: "PPOL670 Final Project"
author: "Ming Zhou & Huixin Cai"
format: html
editor: source
editor_options: 
  chunk_output_type: console
warning: false
self-contained: true
---
# Project Purpose
In this project, we aim to predict whether an individual is in the labor force based on their demographic and economic background characteristics. Hopefully, the model developed in this project can help policymakers identify individuals not in the labor force and provide them with guidance and potential economic support. We expect policymakers to develop a supporting program that identifies those not in the labor force, and 1) interview randomly selected individuals to know better about why they stay out of the labor force; 2) adjust the general budget for job training and economic support programs.
We perform supervised machine learning using multiple models (LASSO, Decision Tree, Random Forest, etc.) both with and without PCA. As our data set is unbalanced, we create a weight variable for adjustment.

# Data
We gathered the variables needed to train the model from the Annual Social and Economic (ASEC) supplement data. The independent variables include individuals’ demographics (e.g., age, education level, marital status, sex, etc.) and other economic status information (e.g., SNAP benefits, EITC benefits, and other social assistance). The Annual Social and Economic (ASEC) Supplement contains the basic monthly demographic and labor force data, plus additional data on work experience, income, noncash benefits, health insurance coverage, and migration. We can access the data through web APIs as the data dictionary linked https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar22.pdf.

```{r set_up, include = FALSE}
library(tidyverse)
library(lubridate)
library(dplyr)
library(tidymodels)
library(tidyclust)
library(factoextra)
library(broom)
library(ggplot2)
library(parsnip)
library(vip)
library(reshape2)
library(patchwork)
library(randomForest)
```

```{r data_loading}
data <- read_csv("Data/pppub22.csv")%>%
  select(A_EXPLF,MIG_REG,SPM_NUMKIDS, SPM_ACTC, SPM_BBSUBVAL,
         SPM_FAMTYPE, SPM_CAPHOUSESUB, SPM_CAPWKCCXPNS, 
         SPM_CHILDCAREXPNS,A_HGA, SPM_CHILDSUPPD, 
         SPM_EITC, A_MARITL, HEA, SPM_FEDTAX, SPM_GEOADJ,
         SPM_ENGVAL, PRDTRACE, PHIP_VAL, MCAID, NOW_MCAID, 
         NOW_DEPNONM,NOW_MRKUN, MRKUN, NOW_MRK, MRK, NOW_DIR,
         OWNDIR, COV, COV_CYR, AGI, EIT_CRED, CHSP_VAL, 
         CSP_VAL, PAW_VAL, RINT_SC2, RINT_VAL1, PTOTVAL, 
         DIS_HP,DIS_CS, DIS_CS, DIS_YN, DIV_YN, DSAB_VAL, 
         PEARNVAL, EARNER, LKWEEKS, LOSEWKS,
         AGE1,A_SEX) %>%
  # exclude observations with missing answers
  filter(complete.cases(.)) %>%
  # only include observations in the labor force (15-64)
  filter(AGE1 > 0 & AGE1 < 15)

# create new variable "in_labor"
# if in the labor force, in_labor equals 1. 
data <- data %>%
  mutate(in_labor = 
           if_else(A_EXPLF == 0,0,1)) %>%
  select(-A_EXPLF)

data$in_labor <- as.factor(data$in_labor)
```

# Supervised modeling without Dimension Reduction 
*Split data*
```{r Split_data}
set.seed(20221111)
split_data <- initial_split(data = data, prop = 0.8)
data_training <- training(x = split_data)
data_testing <- testing(x = split_data)
```

## EDA
### 1) The labor participation by gender
After plotting the labor participation by gender, we find that there are relatively smaller proportion of the sample not in labor force. And fewer females join the labor force compared to male. 
```{r graph_labor_participation_by_gender, fig.keep='all',fig.width=10, fig.height=10}
# the labor participation by gender
data_training %>%
  ggplot(aes(x = in_labor)) +
  geom_bar(aes(fill = factor(A_SEX)), position = "dodge") +
  scale_fill_discrete(name = "Sex",
                      labels = c("Male", 
                                 "Female")) +
  scale_x_discrete(name = "Labor Status",
                   labels = c("Not in labor force", 
                              "In labor force")) +
  scale_y_continuous(name = "The number of individuals") +
  labs(title = "Fewer females join the labor force compared to male",
       caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))
```

### 2) The correlation between each variables
To visualize the correlation between each variables, we create a heat map. The heat map shows the magnitude of the correlation in two dimensions, and the variation in color is designed by intensity. As shown in the heat map, some variables in the data set are highly correlated. For example, PTOTVAL (total person income) and SPM_FEDTAX (Supplemental Poverty Measure unit's Federal tax) are positively correlated, COV (whether individuals have any health insurance coverage last year) and SPM_CAPWKCCXPNS (Supplemental Poverty Measure unit's child care expenses-not capped) are negatively correlated.  
```{r , fig.keep='all',fig.width=10,fig.height=10}
library(reshape2)
library(ggplot2)
# the correlation between each variables
data_training1 <- data_training %>%
  select(-in_labor)

# generate correlation matrix
cormat <- round(cor(data_training1),1)

# get lower triangle of the correlation matrix
get_lower_tri<-function(cormat){
  cormat[upper.tri(cormat)] <- NA
  return(cormat)
}

# get upper triangle of the correlation matrix
get_upper_tri <- function(cormat){
  cormat[lower.tri(cormat)]<- NA
  return(cormat)
}

upper_tri <- get_upper_tri(cormat)

# melt the correlation matrix
melted_cormat <- melt(upper_tri, na.rm = TRUE)
# create a ggheatmap
ggheatmap <- 
  ggplot(melted_cormat, 
         aes(Var2, Var1, fill = value))+
  geom_tile(color = "white")+
  scale_fill_gradient2(low = "blue", high = "red", 
                       mid = "white", midpoint = 0, 
                       limit = c(-1,1), space = "Lab",
                       name="Pearson\nCorrelation") +
  theme_minimal()+ # minimal theme
  theme(axis.text.x = element_text(angle = 90, 
                                   vjust = 1, 
                                   size = 5, 
                                   hjust = 1),
        axis.text.y = element_text(size = 5))+
  coord_fixed()+
  labs(title = "The correlation between variables",
       caption="Data scource: Census.gov",
       x = "variable name" ,
       y = "variable name") 

# print the heat map
print(ggheatmap)
```

### 3) Some variables are highly correlated
To better visualize the high correlation between variables in the data set, we also use geom_smooth and geom_point. As shown in the graph, educational attainment and total persons income are highly correlated. Individuals with higher education are expected to have higher personal income. 
```{r variable_correlation, fig.keep='all',fig.width=10, fig.height=7}
ggplot(data = data_training, 
       aes(x = A_HGA,
           y = PTOTVAL/1000)) +
  geom_point(alpha = 0.1) +
  geom_smooth(method = "auto") +
  labs(title = paste0("Educational attainment",
  " & Income are highly correlated"),
  x = "Educational attainment",
  y= "total persons income ($ Thousand)", 
  caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))
```

### 4) The distribution of variables
After plotting the distribution of some independent variables in our data set, we find that some variables are highly skewed and clustered around some values. 
```{r variable_distribution, fig.keep='all',fig.width=10, fig.height=10}
# the distribution of SPM_ACTC (SPM units Additional Child Tax Credit)
ggplot(data = data_training, 
       aes(x = SPM_ACTC)) +
  geom_histogram(aes(y=..density..),
                 color="darkblue", 
                 fill="light blue") +
  geom_density(alpha=.2, fill="#FF6666") +
  scale_x_continuous(limits = c(0,20000))+
  labs(title = paste0("The distribution of ",
  "Child Tax Credit is skewed & clustered"), 
  x = "SPM units Additional Child Tax Credit",
  y = "density",
  caption="Data scource: Census.gov")+  
  theme(plot.title = element_text(size=10))

# the distribution of SPM_CAPWKCCXPNS 
#(SPM unit's capped work and child care expenses)
ggplot(data = data_training, 
       aes(x = SPM_CAPWKCCXPNS)) +
  geom_histogram(aes(y=..density..),
                 color="dark blue", 
                 fill="light blue") +
  geom_density(alpha=.2, fill="#FF6666") +
  scale_x_continuous(limits = c(0,20000))+
  labs(title = paste0("The distribution of",
  " work and child care expenses is skewed"), 
  x = "SPM unit's capped work and child care expenses ($)",
  caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))
```

## Preprocess
### Add weight
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these observations a much larger weight in the analysis. 
```{r weight}
data_training %>% count(in_labor)

data_training <- data_training %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```

### Recipe
To reduce standard error in the models, we use standardization, removing variables that have large absolute correlations with other variables, and dropping variables with no variance in the pre-process.
```{r recipe}
data_rec <- recipe(in_labor~., data = data_training) %>%
  # center and scale all predictors
  step_normalize(all_numeric_predictors()) %>%
  step_corr(all_numeric_predictors()) %>%
  # drop near zero variance predictors
  step_nzv(all_numeric_predictors()) %>%
  step_zv(all_numeric_predictors())
# see the engineered training data
bake(prep(data_rec, 
          training = data_training),
     new_data = data_training)
```

### Cross validation
```{r cross_validation}
folds <- vfold_cv(data = data_training, v = 10)
```

## Logistic model
The accuracy and sensitivity of the logitstic model is 0.716 and 0, respectively. The logistic model does not fit well in this case. 

In the logistic model, the most important variables are: 
*1)* EARNER: Whether the individual has earned in the household with $1 or more in wages and salaries.
*2)* PEARNVAL:Total persons earnings ($). 
*3)* SPM_CAPWKCCXPNS: SPM unit's capped work and child care expenses ($). 
*4)* DIS_HP: Whether the individual has a health problem or a disability which prevents work or which limits the kind or amount of work. 
*5)* AGE1: Recoded age, (0 = Not in universe, 1 = 15 years, 2 = 16 and 17 years, 3 = 18 and 19 years, 4 = 20 and 21 years, 5 = 22 to 24 years, 6 = 25 to 29 years, 7 = 30 to 34 years, 8 = 35 to 39 years, 9 = 40 to 44 years, 10 = 45 to 49 years, 11 = 50 to 54 years, 12 = 55 to 59 years, 13 = 60 to 61 years, 14 = 62 to 64 years, 15 = 65 to 69 years, 16 = 70 to 74 years, 17 = 75 years and over.)
*6)* SPM_FEDTAX: SPM unit's Federal tax ($).
*7)* SPM_FAMTYPE: SPM unit's family type. 
*8)* A_HGA: Educational attainment. 
*9)* DIV_YN: Whether the individual received dividends. 
*10)* COV: Whether the individual had any health insurance coverage last year. 

```{r logistic}
# create model
logistic_mod <- 
  logistic_reg(penalty = 1) %>%
  set_engine("glmnet") %>%
  set_mode("classification") 

# create workflow
logistic_w <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(logistic_mod) %>%
  add_case_weights(case_wts) 

# perform cross validation 
logistic_cv <- logistic_w %>%
  fit_resamples(resamples = folds)

# select the best model based on the "accuracy" metric
logistic_best <- logistic_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
logistic_final <- 
  finalize_workflow(logistic_w,
                    parameters = logistic_best)

# fit model to the training data
logistic_fit <- logistic_final %>%
  fit(data = data_training)

# make predictions
predictions_logistic  <- bind_cols(
  data_training,
  predict(object = logistic_fit, 
          new_data = data_training),
  predict(object = logistic_fit, 
          new_data = data_training, 
          type = "prob"))

# generate confusion matrix
cm_logistic <- conf_mat(data = predictions_logistic,
               truth = in_labor,
               estimate = .pred_class)
cm_logistic

accuracy(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_logistic,
         truth = in_labor,
         estimate = .pred_class)

# variable importance 
logistic_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10, 
      aesthetics = list(color = "dark blue", 
                        fill = "light blue"))
```

## LASSO Model 
The accuracy and sensitivity of the LASSO model is 0.929 and 0.850, respectively. 

In the LASSO model, the most important variables are (the same as those in the logistic model): 
*1)* EARNER: Whether the individual has earned in the household with $1 or more in wages and salaries.
*2)* PEARNVAL:Total persons earnings ($). 
*3)* SPM_CAPWKCCXPNS: SPM unit's capped work and child care expenses ($). 
*4)* DIS_HP: Whether the individual has a health problem or a disability which prevents work or which limits the kind or amount of work. 
*5)* AGE1: Recoded age, (0 = Not in universe, 1 = 15 years, 2 = 16 and 17 years, 3 = 18 and 19 years, 4 = 20 and 21 years, 5 = 22 to 24 years, 6 = 25 to 29 years, 7 = 30 to 34 years, 8 = 35 to 39 years, 9 = 40 to 44 years, 10 = 45 to 49 years, 11 = 50 to 54 years, 12 = 55 to 59 years, 13 = 60 to 61 years, 14 = 62 to 64 years, 15 = 65 to 69 years, 16 = 70 to 74 years, 17 = 75 years and over.)
*6)* SPM_FEDTAX: SPM unit's Federal tax ($).
*7)* SPM_FAMTYPE: SPM unit's family type. 
*8)* A_HGA: Educational attainment. 
*9)* DIV_YN: Whether the individual received dividends. 
*10)* COV: Whether the individual had any health insurance coverage last year.

```{r LASSO}
# create a tuning grid for lasso regularization
lasso_grid <- grid_regular(penalty(), levels = 50)

# create a linear_regression model 
# so that we can tune the penalty parameter
# set the mixture parameter to 1 
# and use "glmnet" for the engine
lasso_mod <- 
  logistic_reg(penalty = tune(),
                          mixture = 1) %>%
  set_engine("glmnet")

# create a workflow using logistic regression model
lasso_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(lasso_mod) %>%
  add_case_weights(case_wts)

# perform hyperparameter tuning
lasso_cv <- lasso_wf %>%
  tune_grid(
    resamples = folds,
    grid = lasso_grid)

# select the best model based on the "rmse" metric
lasso_best <- lasso_cv %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final <- finalize_workflow(
  lasso_wf,
  parameters = lasso_best)

#fit model to the training data
lasso_fit <- lasso_final %>%
  fit(data = data_training) 

#make predictions
predictions_lasso  <- bind_cols(
  data_training,
  predict(object = lasso_fit, 
          new_data = data_training),
  predict(object = lasso_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_lasso <- conf_mat(data = predictions_lasso,
               truth = in_labor,
               estimate = .pred_class)

cm_lasso

accuracy(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
lasso_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10,
      aesthetics = list(color = "dark blue", 
                        fill = "light blue"))
```

## Compare LASSO and logistic model
The standard error of the LASSO model's accuracy is much lower than the logistic's, and the mean of LASSO model's accuracy is much higher than the logistic's. The LASSO model is far better than the logistic model, fitting well in the training data with high accuracy and sensitivity. 

LASSO model has higher precision in this case. 

```{r compare_LASSO_logistic}
LASSO_Logistic <- bind_rows(
  `logistic` = show_best(logistic_cv, 
                         metric = "accuracy", 
                         n = 1),
  `LASSO` = show_best(lasso_cv, 
                      metric = "accuracy", 
                      n = 1),
  .id = "model")

LASSO_Logistic %>%
  select(model,mean,std_err)
```

## Decision Tree 
The accuracy and sensitivity of the Decision Tree model is 0.922 and 0.879, respectively. The Decision Tree model fits the training data as well as LASSO model, with slightly higher sensitivity. 

In the Decision Tree model, the most important variables are: 
*1)* PEARNVAL:Total persons earnings ($). 
*2)* EARNER: Whether the individual has earned in the household with $1 or more in wages and salaries.
*3)* AGI: Federal adjusted gross income ($).
*4)* SPM_CAPWKCCXPNS: SPM unit's capped work and child care expenses ($). 
*5)* A_HGA: Educational attainment. 
*6)* AGE1: Recoded age, (0 = Not in universe, 1 = 15 years, 2 = 16 and 17 years, 3 = 18 and 19 years, 4 = 20 and 21 years, 5 = 22 to 24 years, 6 = 25 to 29 years, 7 = 30 to 34 years, 8 = 35 to 39 years, 9 = 40 to 44 years, 10 = 45 to 49 years, 11 = 50 to 54 years, 12 = 55 to 59 years, 13 = 60 to 61 years, 14 = 62 to 64 years, 15 = 65 to 69 years, 16 = 70 to 74 years, 17 = 75 years and over.)

```{r decision_tree}
#create model
tree_mod <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

#create workflow
tree_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(tree_mod) %>%
  add_case_weights(case_wts)

#perform cross validation 
tree_cv <- tree_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "rmse" metric
tree_best <- tree_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
tree_final <- finalize_workflow(
  tree_wf,
  parameters = tree_best)

#fit model to the training data
tree_fit <- tree_final %>%
  fit(data = data_training) 

#make predictions 
predictions_tree  <- bind_cols(
  data_training,
  predict(object = tree_fit, 
          new_data = data_training),
  predict(object = tree_fit, 
          new_data = data_training, 
          type = "prob"))

#generate comfusion matrix
cm_tree <- conf_mat(data = predictions_tree,
               truth = in_labor,
               estimate = .pred_class)
cm_tree

accuracy(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
tree_fit %>%
  extract_fit_parsnip() %>%
  vip(num_features = 10,
      aesthetics = list(color = "dark blue", 
                        fill = "light blue"))

```

## Random Forest 
The accuracy and sensitivity of the Random Forest model is 0.969 and 0.984, respectively. The Random Forest model fits the training data best among the four models.

In the Random Forest model, the most important variables are: 
*1)* PEARNVAL:Total persons earnings ($). 
*2)* EARNER: Whether the individual has earned in the household with $1 or more in wages and salaries.
*3)* AGI: Federal adjusted gross income ($).
*4)* SPM_CAPWKCCXPNS: SPM unit's capped work and child care expenses ($). 
 DIS_HP: Whether the individual has a health problem or a disability which prevents work or which limits the kind or amount of work. 
*5)* SPM_FEDTAX: SPM unit's Federal tax ($).
*6)* AGE1: Recoded age, (0 = Not in universe, 1 = 15 years, 2 = 16 and 17 years, 3 = 18 and 19 years, 4 = 20 and 21 years, 5 = 22 to 24 years, 6 = 25 to 29 years, 7 = 30 to 34 years, 8 = 35 to 39 years, 9 = 40 to 44 years, 10 = 45 to 49 years, 11 = 50 to 54 years, 12 = 55 to 59 years, 13 = 60 to 61 years, 14 = 62 to 64 years, 15 = 65 to 69 years, 16 = 70 to 74 years, 17 = 75 years and over.)
*7)* SPM_GEOADJ: SPM unit's geographic food, shelter, clothing and utility (FSCU) adjustment.
*8)* A_HGA: Educational attainment. 
*9)* RINT_VAL1: The interest income amount (retirement source with the value of 1). 
*10)* A_MARITL: The individual's marital status. 

```{r Random_forest}
set.seed(20221111)

#create model
rf_mod <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger",importance = "impurity") 

#create workflow
rf_wf <- workflow() %>%
  add_recipe(data_rec) %>%
  add_model(rf_mod) %>%
  add_case_weights(case_wts)

#perform cross validation
rf_cv <- rf_wf %>%
  fit_resamples(resamples = folds)

#select the best model based on the "accuracy" metric
rf_best <- rf_cv %>%
  select_best(metric = "accuracy")

#finalize workflow
rf_final <- 
  finalize_workflow(rf_wf,
                    parameters = rf_best)

#fit model to the training data
rf_fit <- rf_final %>%
  fit(data = data_training)

# make predictions
predictions_rf  <- bind_cols(
  data_training,
  predict(object = rf_fit, 
          new_data = data_training),
  predict(object = rf_fit, 
          new_data = data_training, type = "prob"))

#generate confusion matrix
cm_rf <- conf_mat(data = predictions_rf,
               truth = in_labor,
               estimate = .pred_class)
cm_rf

accuracy(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf,
         truth = in_labor,
         estimate = .pred_class)

#variable importance 
ranger_obj <- pull_workflow_fit(rf_fit)$fit
vip(ranger_obj,
    geom = "col",
    aesthetics = list(color = "dark blue", 
                      fill = "light blue"))
```

## Summary - Model without PCA 
In terms of model accuracy and sensitivity, Random Forest model dose the best job in predicting individual's labor status. 

Personal income, the work and child total expenses, education attainment, and the marital status are the most importance variables in predicting individual's labor status. After plotting each variable with labor status, we find:
*1)* Individuals with higher income are more likely to be in labor force. 
*2)* Individuals with higher child care expenses are more likely to be in labor force. 
*3)* Individuals with higher education attainment are more likely to be in labor force. 
*4)* Individuals with difference marital status have statistical significance in labor force participation. 
```{r model_comparison_&_variable_importance_visual, fig.keep='all',fig.width=10, fig.height=10 }
models <- data.frame(
  models = c("logistic",
             "LASSO",
             "Decision_Tree",
             "Random_Forest"),
  accuracy = c(0.716,0.929,0.922,0.969),
  sensitivity = c(0,0.850,0.879,0.984)
  )

models

# PEARNVAL and in_labor 
g1 <- ggplot(data = data_training) +
  geom_point(aes(x = in_labor, y = PEARNVAL), 
             alpha = 0.2) +
  scale_x_discrete(name ="Labor Status",
                  labels = c("Not in labor force", 
                              "In labor force")) +
  labs(title = "Positive effects of income on labor status",
       x = "labor status",
       y= "total persons income ($ Thousand)", 
       caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))

#SPM_CAPWKCCXPNS and in_labor 
g2 <- ggplot(data = data_training) +
  geom_point(aes(x = in_labor, y = SPM_CAPWKCCXPNS),
             alpha = 0.2) +
  scale_x_discrete(name ="Labor Status",
                  labels = c("Not in labor force", 
                              "In labor force")) +
  labs(title = paste0("Positive effects",
  " of work and child care expenses on labor status"),
  x = "labor status",
  y= paste0("SPM unit's capped work",
  "and child care expenses ($ Thousand)"),  
  caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10)) 

#A_HGA and in_labor 
g3 <- ggplot(data = data_training) +
  geom_bar(aes(x = A_HGA, fill = in_labor)) + 
  scale_fill_discrete(name ="labor status",
                   labels = c("Not in labor force", 
                              "In labor force")) +
  labs(title = paste0("Positive effects",
  "of education attainment on labor status"),
       x = "Education attainment", 
       caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))

#A_MARITL and in_labor 
g4 <- ggplot(data = data_training) +
  geom_bar(aes(x = A_MARITL, fill = in_labor)) + 
  scale_fill_discrete(name = "Labor Status",
                   labels = c("Not in labor force", 
                              "In labor force")) +
  scale_x_discrete(name = "The individual's marital status",
                   limits = c("Married_civilian",
                              "Married_AF",
                              "Married_absent",
                              "Widowed",
                              "Divorced",
                              "Separated",
                              "Never married")) +
  theme(axis.text.x = element_text(angle = 30)) + 
  labs(
    title = "Effects of marital status on labor status",
    caption="Data scource: Census.gov") +
  theme(plot.title = element_text(size=10))

g1 + g2

g4 + g3
```

# Dimension Reduction
## Decide the number of principle components
```{r number_of_PC}
data1 <- data %>%
  select(-in_labor)
# create PCA
set.seed(20221111)
pca_1 <- prcomp(data1, scale = TRUE)
# create scree plot to decide how many pc to retain 
screeplot(pca_1, type="lines")
```
The scree plot occurs at component 4, which is the “elbow” of the scree plot. Therefore, it cound be argued based on the basis of the scree plot that **the first three components** should be retained.

## Dimension Reducation with PCA
```{r dimension_reduction}
# create a recipe with no outcome variable 
# and all predictors
preprocess_rec <- recipe(~ ., data = data1) %>%
  step_zv(all_numeric_predictors()) %>%
  step_nzv(all_numeric_predictors()) %>%
  # center and scale (normalize) all predictors
  step_normalize(all_numeric_predictors())%>%
  # perform pca and use num_comp = 3 keeping
  #three components
  step_pca(all_numeric_predictors(),num_comp = 3) %>%
  # run prep to prepare recipe
  prep()

# apply recipe to data
processed_data <- preprocess_rec %>%
  bake(new_data = data1)

#combine variables
data_pca <- cbind(
  data$in_labor, processed_data
)

# rename variable "data$A_EXPLF"
data_pca <- rename(data_pca,
       "in_labor" = "data$in_labor")
```

# Supervised Modeling with Dimension Reduction
## Preprocess
```{r split_data_pca}
set.seed(20221111)
split_pca <- initial_split(data = data_pca, 
                           prop = 0.8)
data_training_pca <- training(x = split_pca)
data_testing_pca <- testing(x = split_pca)
```

### Add weight
Since we would like to encourage models to more accurately predict the individuals who are not in the labor force, we can give these samples a much larger weight in the analysis. 
```{r weight_pca}
data_training_pca %>% count(in_labor)

data_training_pca <- data_training_pca %>%
    mutate(
      case_wts = if_else(in_labor == "0", 2.5, 1),
      case_wts = importance_weights(case_wts))
```

### Cross validation
```{r cv_pca}
folds_pca <- vfold_cv(data = data_training_pca,
                      v = 10)
```

### Recipe
To reduce standard error in the models, we use standardization and removing variables that have large absolute correlations with other variables in the pre-process.
```{r rec_pca}
#Build a recipe 
data_rec_pca <- recipe(
  in_labor~PC1+PC2+PC3+case_wts, 
  data = data_training_pca) %>%
  # center and scale all predictors
  step_normalize(all_predictors()) %>%
  step_corr(all_predictors())
```

## LASSO Model with PCA
The accuracy and sensitivity of the LASSO model with PCA is 0.785 and 0.782, respectively, which fit the training data worse than the LASSO model without PCA.
```{r lasso_pca}
# create a tuning grid for lasso regularization,
lasso_grid_pca <- grid_regular(penalty(), 
                               levels = 50)

# create a logistic_regression model with tuning
lasso_mod_pca <- logistic_reg(penalty = tune(), 
                              mixture = 1) %>%
  set_engine("glmnet")

# create workflow
lasso_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(lasso_mod_pca) %>%
  add_case_weights(case_wts) 

# perform hyperparameter tuning 
lasso_cv_pca <- lasso_wf_pca %>%
  tune_grid(
    resamples = folds_pca,
    grid = lasso_grid_pca)

# select the best model based on the "rmse" metric
lasso_best_pca <- lasso_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
lasso_final_pca <- finalize_workflow(
  lasso_wf_pca,
  parameters = lasso_best_pca)

# fit the model to the training data 
lasso_fit_pca <- lasso_final_pca %>%
  fit(data = data_training_pca) 

# make predictions
predictions_lasso_pca  <- bind_cols(
  data_training_pca,
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca),
  predict(object = lasso_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

# generate confusion matrix 
cm_lasso_pca <- conf_mat(
  data = predictions_lasso_pca,
  truth = in_labor,
  estimate = .pred_class)

cm_lasso_pca 

accuracy(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_lasso_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Decision tree with PCA
The accuracy and sensitivity of the Decision Tree model with PCA is 0.820 and 0.743, respectively, which fit the training data worse than the Decision Tree model without PCA.
```{r decision_tree_pca}
# create model
tree_mod_pca <-
  decision_tree() %>%
  set_engine(engine = "rpart") %>%
  set_mode(mode = "classification")

# create workflow
tree_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(tree_mod_pca) %>%
  add_case_weights(case_wts) 

# perform cross validation 
tree_cv_pca <- tree_wf_pca %>%
  fit_resamples(resamples = folds_pca)

# select the best model based on the "accuracy" metric
tree_best_pca <- tree_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
tree_final_pca <- finalize_workflow(
  tree_wf_pca,
  parameters = tree_best_pca)

# fit the model to the training data 
tree_fit_pca <- tree_final_pca %>%
  fit(data = data_training_pca) 

# make predictions
predictions_tree_pca  <- bind_cols(
  data_training_pca,
  predict(object = tree_fit_pca, 
          new_data = data_training_pca),
  predict(object = tree_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

# generate confusion matrix
cm_tree_pca <- conf_mat(data = predictions_tree_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_tree_pca

accuracy(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_pca,
         truth = in_labor,
         estimate = .pred_class)
```

## Random Forest with PCA
The accuracy and sensitivity of the Random Forest model with PCA is 0.961 and 0.999, respectively, which fit the training data better than the Random Forest model without PCA.
```{r rf_pca}
set.seed(20221111)
# create model
rf_mod_pca <- rand_forest(trees = 100) %>%
  set_mode("classification") %>%
  set_engine("ranger", importance = "impurity")

# create workflow
rf_wf_pca <- workflow() %>%
  add_recipe(data_rec_pca) %>%
  add_model(rf_mod_pca) %>%
  add_case_weights(case_wts) 

# perform cross validation
rf_cv_pca <- rf_wf_pca %>%
  fit_resamples(resamples = folds_pca)

# select the best model based on the "accuracy" metric
rf_best_pca <- rf_cv_pca %>%
  select_best(metric = "accuracy")

# finalize workflow
rf_final_pca <- 
  finalize_workflow(rf_wf_pca,
                    parameters = rf_best_pca)

# fit the model to the training data
rf_fit_pca <- rf_final_pca %>%
  fit(data = data_training_pca)

# make predictions
predictions_rf_pca  <- bind_cols(
  data_training_pca,
  predict(object = rf_fit_pca, 
          new_data = data_training_pca),
  predict(object = rf_fit_pca, 
          new_data = data_training_pca, 
          type = "prob"))

# generate confusion matrix 
cm_rf_pca <- conf_mat(data = predictions_rf_pca,
               truth = in_labor,
               estimate = .pred_class)
cm_rf_pca

accuracy(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_pca,
         truth = in_labor,
         estimate = .pred_class)
```
## Summary - Model with PCA 
In terms of model accuracy and sensitivity, Random Forest model with PCA dose the best job in predicting individual's labor status. 

```{r compare_models_with_PCA}
models_pca <- data.frame(
  models = c("LASSO",
             "Decision_Tree",
             "Random_Forest"),
  accuracy = c(0.785,0.820,0.961),
  sensitivity = c(0.782,0.743,0.999)
  )

models_pca
```

# Final model: Decision Tree and Random Forest with PCA
The accuracy and sensitivity of the Random Forest model with PCA is highest among the seven models and Decision Tree model shows relative high sensitivity among models without PCA, as shown in the plot. We choose one model with PCA (Random Forest) and one without PCA (Decision Tree) as our final models to fit the testing data. 
```{r compare_models, fig.keep='all',fig.width=10, fig.height=10}
models_all <- data.frame(
  models = c("Logistic",
             "LASSO",
             "Decision_Tree",
             "Random_Forest",
             "LASSO_PCA",
             "Decision_Tree_PCA",
             "Random_Forest_PCA"),
  accuracy = c(0.716,0.929,0.922,0.969,
               0.785,0.820,0.961),
  sensitivity = c(0,0.850,0.879,0.984,
                  0.782,0.743,0.999)
  )

highight <- 
  models_all %>%
  filter(models == "Decision_Tree" |
           models == "Random_Forest_PCA")
  
models_all %>%
  ggplot() +  
  geom_text(aes(x = accuracy, 
                y = sensitivity, 
                label = models),
            check_overlap = TRUE,
            nudge_x = -0.007,
            nudge_y = -0.01,
            size = 3,
            angle = 45,
            fontface = "bold") +
  geom_point(aes(x = accuracy, y = sensitivity),
             size = 3) +
  geom_point(data = highight,
             aes(x = accuracy, y = sensitivity),
             color = "red",
             size = 3) +
  labs(title =
         paste0("Decision tree and random forest with PCA",
                " have relatively high accuracy and sensitivity"))
```

## Final model 1: Random forest with PCA 
The Random Forest Model fits testing data well, as shown in the density plot. The accuracy and sensitivity of the Random Forest model with PCA is relatively high in the testing data, with 0.866 and 0.810 respectively. 
```{r final_model_1}
# fit the model to the testing data and make predictions
predictions_rf_test_pca <- bind_cols(
  data_testing_pca,
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca),
  predict(object = rf_fit_pca, 
          new_data = data_testing_pca, type = "prob"))

#generate confusion matrix 
cm_rf_test_pca <- conf_mat(
  data = predictions_rf_test_pca,
  truth = in_labor,
  estimate = .pred_class)

cm_rf_test_pca

accuracy(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_rf_test_pca,
         truth = in_labor,
         estimate = .pred_class)

#plot distributions of the predicted probability 
#distributions for each class 
predictions_rf_test_pca %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = paste0("Sensitivity is high in the model",
  " of random forest with PCA"), 
       x = paste0("The probability of",
       " being predicted not in labor force"))
```

## Final model 2: Decision tree 
```{r final_model_2}
# fit the model to the testing data and make predictions
predictions_tree_test <- bind_cols(
  data_testing,
  predict(object = tree_fit, 
          new_data = data_testing),
  predict(object = tree_fit, 
          new_data = data_testing, type = "prob"))

# generate confusion matrix 
cm_tree_test <- conf_mat(data = predictions_tree_test,
               truth = in_labor,
               estimate = .pred_class)

cm_tree_test

accuracy(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

sensitivity(data = predictions_tree_test,
         truth = in_labor,
         estimate = .pred_class)

# plot distributions of the predicted probability distributions for each class 
predictions_tree_test %>%
  ggplot() +
  geom_density(aes(x = .pred_0, fill = in_labor), 
               alpha = 0.5)+
  scale_fill_discrete(name = "Whether in labor force",
                      labels = c("not in labor force",
                                 "in labor force"))+
  labs(title = paste0("Sensitivity and accuracy",
  " are both significantly high in the decision tree model"), 
       x = paste0("The probability of",
       "being predicted not in labor force"))
```

# Limitations
The independent variables in our supervised modeling are manually selected by us, focusing on demographic and basic economic characteristics. If technically permitted, training a model using all of the variables in our data set may result in better performance.
We choose Random Forest as our final model. We generate only 100 trees, while 1000 trees is commonly recommended to generate an accurate model. Although our Random Forest model with PCA reaches an accuracy of 0.999 when fitting to the training data, its accuracy drops when fitting to the testing data. A model with a larger number of trees may be able to do a better job.
Both Random Forest models (with and without PCA) reach an accuracy of nearly 1 during training. However, the performance of both models drops when fitting to the testing data, which raises our concerns about potential over-fitting problem.

# Bibliography
1. Rebecca Barter, "Tidymodels: tidy machine learning in R", https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
2. Max Kuhn, "Using case weights with tidymodels", https://www.tidyverse.org/blog/2022/05/case-weights/
3. STHDA, http://www.sthda.com/english/wiki/ggplot2-quick-correlation-matrix-heatmap-r-software-and-data-visualization
4. "Breiman and Cutler's Random Forests for Classification and Regression", https://cran.r-project.org/web/packages/randomForest/randomForest.pdf
5. Cian White, "Principal Component Analysis in R", https://rstudio-pubs-static.s3.amazonaws.com/585948_abd70a6fc3e24d4fad8944197bc5dd25.html
6. Rebecca Barter, "Tidymodels: tidy machine learning in R", https://www.rebeccabarter.com/blog/2020-03-25_machine_learning/
